{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsm-UOC/tensorflowCourse/blob/main/FoodVisionMilestoneProject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2PPYrQIztfX"
      },
      "source": [
        "# Note: This is the template version of Notebook 07\n",
        "\n",
        "Your goal is to fill out all of the code in sections labelled \"TODO\" (search for \"TODO\").\n",
        "\n",
        "There will often be more than one way to complete the code challenges (don't overthink this, just write code to do what the comments say, this might require researching/looking for examples, this is normal).\n",
        "\n",
        "You can find an example of the \"TODO\" sections filled out in the original [Food Vision Milestone Project 1 notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/07_food_vision_milestone_project_1.ipynb).\n",
        "\n",
        "Everything below the following line is from said notebook.\n",
        "\n",
        "If you have any questions, refer to the GitHub discussions page: https://github.com/mrdbourke/tensorflow-deep-learning/discussions\n",
        "\n",
        "----\n",
        "\n",
        "# Milestone Project 1: 🍔👁 Food Vision Big™\n",
        "\n",
        "In the previous notebook ([transfer learning part 3: scaling up](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb)) we built Food Vision mini: a transfer learning model which beat the original results of the [Food101 paper](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) with only 10% of the data.\n",
        "\n",
        "But you might be wondering, what would happen if we used all the data?\n",
        "\n",
        "Well, that's what we're going to find out in this notebook!\n",
        "\n",
        "We're going to be building Food Vision Big™, using all of the data from the Food101 dataset.\n",
        "\n",
        "Yep. All 75,750 training images and 25,250 testing images.\n",
        "\n",
        "And guess what...\n",
        "\n",
        "This time **we've got the goal of beating [DeepFood](https://www.researchgate.net/publication/304163308_DeepFood_Deep_Learning-Based_Food_Image_Recognition_for_Computer-Aided_Dietary_Assessment)**, a 2016 paper which used a Convolutional Neural Network trained for 2-3 days to achieve 77.4% top-1 accuracy.\n",
        "\n",
        "> 🔑 **Note:** **Top-1 accuracy** means \"accuracy for the top softmax activation value output by the model\" (because softmax ouputs a value for every class, but top-1 means only the highest one is evaluated). **Top-5 accuracy** means \"accuracy for the top 5 softmax activation values output by the model\", in other words, did the true label appear in the top 5 activation values? Top-5 accuracy scores are usually noticeably higher than top-1.\n",
        "\n",
        "|  | 🍔👁 Food Vision Big™ | 🍔👁 Food Vision mini |\n",
        "|-----|-----|-----|\n",
        "| Dataset source | TensorFlow Datasets | Preprocessed download from Kaggle |\n",
        "| Train data | 75,750 images | 7,575 images |\n",
        "| Test data | 25,250 images | 25,250 images |\n",
        "| Mixed precision | Yes | No |\n",
        "| Data loading | Performanant tf.data API | TensorFlow pre-built function |  \n",
        "| Target results | 77.4% top-1 accuracy (beat [DeepFood paper](https://arxiv.org/abs/1606.05675)) | 50.76% top-1 accuracy (beat [Food101 paper](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf)) |\n",
        "\n",
        "*Table comparing difference between Food Vision Big (this notebook) versus Food Vision mini (previous notebook).*\n",
        "\n",
        "Alongside attempting to beat the DeepFood paper, we're going to learn about two methods to significantly improve the speed of our model training:\n",
        "1. Prefetching\n",
        "2. Mixed precision training\n",
        "\n",
        "But more on these later.\n",
        "\n",
        "## What we're going to cover\n",
        "\n",
        "* Using TensorFlow Datasets to download and explore data\n",
        "* Creating preprocessing function for our data\n",
        "* Batching & preparing datasets for modelling (**making our datasets run fast**)\n",
        "* Creating modelling callbacks\n",
        "* Setting up **mixed precision training**\n",
        "* Building a feature extraction model (see [transfer learning part 1: feature extraction](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb))\n",
        "* Fine-tuning the feature extraction model (see [transfer learning part 2: fine-tuning](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb))\n",
        "* Viewing training results on TensorBoard\n",
        "\n",
        "## How you should approach this notebook\n",
        "\n",
        "You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.\n",
        "\n",
        "Write all of the code yourself.\n",
        "\n",
        "Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?\n",
        "\n",
        "You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.\n",
        "\n",
        "Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.\n",
        "\n",
        "> 📖 **Resource:** See the full set of course materials on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLaDq25mykWN"
      },
      "source": [
        "## Check GPU\n",
        "\n",
        "For this notebook, we're going to be doing something different.\n",
        "\n",
        "We're going to be using mixed precision training.\n",
        "\n",
        "Mixed precision training was introduced in [TensorFlow 2.4.0](https://blog.tensorflow.org/2020/12/whats-new-in-tensorflow-24.html) (a very new feature at the time of writing).\n",
        "\n",
        "What does **mixed precision training** do?\n",
        "\n",
        "Mixed precision training uses a combination of single precision (float32) and half-preicison (float16) data types to speed up model training (up 3x on modern GPUs).\n",
        "\n",
        "We'll talk about this more later on but in the meantime you can read the [TensorFlow documentation on mixed precision](https://www.tensorflow.org/guide/mixed_precision) for more details.\n",
        "\n",
        "For now, before we can move forward if we want to use mixed precision training, we need to make sure the GPU powering our Google Colab instance (if you're using Google Colab) is compataible.\n",
        "\n",
        "For mixed precision training to work, you need access to a GPU with a compute compability score of 7.0+.\n",
        "\n",
        "Google Colab offers P100, K80 and T4 GPUs, however, **the P100 and K80 aren't compatible with mixed precision training**.\n",
        "\n",
        "Therefore before we proceed we need to make sure we have **access to a Tesla T4 GPU in our Google Colab instance**.\n",
        "\n",
        "If you're not using Google Colab, you can find a list of various [Nvidia GPU compute capabilities on Nvidia's developer website](https://developer.nvidia.com/cuda-gpus#compute).\n",
        "\n",
        "> 🔑 **Note:** If you run the cell below and see a P100 or K80, try going to to Runtime -> Factory Reset Runtime (note: this will remove any saved variables and data from your Colab instance) and then retry to get a T4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAC_5rYJicZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38754ea6-d60c-4df6-bcdb-dd6cffd16129"
      },
      "source": [
        "# If using Google Colab, this should output \"Tesla T4\" otherwise,\n",
        "# you won't be able to use mixed precision training\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-68a35eda-bf67-3baf-df89-52c2a2c946e9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oGXNJWlbG1d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWgb38BYKhS_"
      },
      "source": [
        "Since mixed precision training was introduced in TensorFlow 2.4.0, make sure you've got at least TensorFlow 2.4.0+."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpEDWLxKg46",
        "outputId": "31bca64d-8c0e-45e5-8928-c8156c5181af"
      },
      "source": [
        "# Check TensorFlow version (should be 2.4.0+)\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPwSfuFDzT5v"
      },
      "source": [
        "## Get helper functions\n",
        "\n",
        "We've created a series of helper functions throughout the previous notebooks in the course. Instead of rewriting them (tedious), we'll import the [`helper_functions.py`](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py) file from the GitHub repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC2R6bOZzhQd",
        "outputId": "dccd5c71-9a23-4c75-aa35-dc2428443e0d"
      },
      "source": [
        "# Get helper functions file\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-11 14:32:32--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-11 14:32:33 (76.8 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqKKuFt7zYvf"
      },
      "source": [
        "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5BE7WYl9b_8"
      },
      "source": [
        "## Use TensorFlow Datasets to Download Data\n",
        "\n",
        "In previous notebooks, we've downloaded our food images (from the [Food101 dataset](https://www.kaggle.com/dansbecker/food-101/home)) from Google Storage.\n",
        "\n",
        "And this is a typical workflow you'd use if you're working on your own datasets.\n",
        "\n",
        "However, there's another way to get datasets ready to use with TensorFlow.\n",
        "\n",
        "For many of the most popular datasets in the machine learning world (often referred to and used as benchmarks), you can access them through [TensorFlow Datasets (TFDS)](https://www.tensorflow.org/datasets/overview).\n",
        "\n",
        "What is **TensorFlow Datasets**?\n",
        "\n",
        "A place for prepared and ready-to-use machine learning datasets.\n",
        "\n",
        "Why use TensorFlow Datasets?\n",
        "\n",
        "* Load data already in Tensors\n",
        "* Practice on well established datasets\n",
        "* Experiment with differet data loading techniques (like we're going to use in this notebook)\n",
        "* Experiment with new TensorFlow features quickly (such as mixed precision training)\n",
        "\n",
        "Why *not* use TensorFlow Datasets?\n",
        "\n",
        "* The datasets are static (they don't change, like your real-world datasets would)\n",
        "* Might not be suited for your particular problem (but great for experimenting)\n",
        "\n",
        "To begin using TensorFlow Datasets we can import it under the alias `tfds`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDMExkAG8ztE"
      },
      "source": [
        "# Get TensorFlow Datasets\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TRPTGvpNuJm"
      },
      "source": [
        "To find all of the available datasets in TensorFlow Datasets, you can use the `list_builders()` method.\n",
        "\n",
        "After doing so, we can check to see if the one we're after (`\"food101\"`) is present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXA8b2619s0X",
        "outputId": "ccf07522-289e-413b-bdff-27a613f660f0"
      },
      "source": [
        "# List available datasets\n",
        "datasets_list = tfds.list_builders() # get all available datasets in TFDS\n",
        "print(\"food101\" in datasets_list) # is the dataset we're after available?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUK_zulYNfVY"
      },
      "source": [
        "Beautiful! It looks like the dataset we're after is available (note there are plenty more available but we're on Food101).\n",
        "\n",
        "To get access to the Food101 dataset from the TFDS, we can use the [`tfds.load()`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) method.\n",
        "\n",
        "In particular, we'll have to pass it a few parameters to let it know what we're after:\n",
        "* `name` (str) : the target dataset (e.g. `\"food101\"`)\n",
        "* `split` (list, optional) : what splits of the dataset we're after (e.g. `[\"train\", \"validation\"]`)\n",
        "  * the `split` parameter is quite tricky. See [the documentation for more](https://github.com/tensorflow/datasets/blob/master/docs/splits.md).\n",
        "* `shuffle_files` (bool) : whether or not to shuffle the files on download, defaults to `False`\n",
        "* `as_supervised` (bool) : `True` to download data samples in tuple format (`(data, label)`) or `False` for dictionary format\n",
        "* `with_info` (bool) : `True` to download dataset metadata (labels, number of samples, etc)\n",
        "\n",
        "> 🔑 **Note:** Calling the `tfds.load()` method will start to download a target dataset to disk if the `download=True` parameter is set (default). This dataset could be 100GB+, so make sure you have space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClXZDWng-s8F"
      },
      "source": [
        "# Load in the data (takes about 5-6 minutes in Google Colab)\n",
        "(train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS\n",
        "                                             split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n",
        "                                             shuffle_files=True, # shuffle files on download?\n",
        "                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n",
        "                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSxo6soUwTQl"
      },
      "source": [
        "Wonderful! After a few minutes of downloading, we've now got access to entire Food101 dataset (in tensor format) ready for modelling.\n",
        "\n",
        "Now let's get a little information from our dataset, starting with the class names.\n",
        "\n",
        "Getting class names from a TensorFlow Datasets dataset requires downloading the \"`dataset_info`\" variable (by using the `as_supervised=True` parameter in the `tfds.load()` method, **note:** this will only work for supervised datasets in TFDS).\n",
        "\n",
        "We can access the class names of a particular dataset using the `dataset_info.features` attribute and accessing `names` attribute of the the `\"label\"` key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zoy8Tu7VR2ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8e123c-a3fd-4055-c669-f851725a9206"
      },
      "source": [
        "# Features of Food101 TFDS\n",
        "ds_info.features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeaturesDict({\n",
              "    'image': Image(shape=(None, None, 3), dtype=uint8),\n",
              "    'label': ClassLabel(shape=(), dtype=int64, num_classes=101),\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2UkCaLsDXaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc8cbb17-56f9-47cd-cc3c-7924e37ae954"
      },
      "source": [
        "# Get class names\n",
        "class_names = ds_info.features[\"label\"].names\n",
        "class_names[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple_pie',\n",
              " 'baby_back_ribs',\n",
              " 'baklava',\n",
              " 'beef_carpaccio',\n",
              " 'beef_tartare',\n",
              " 'beet_salad',\n",
              " 'beignets',\n",
              " 'bibimbap',\n",
              " 'bread_pudding',\n",
              " 'breakfast_burrito']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwsBAkGKwh08"
      },
      "source": [
        "### Exploring the Food101 data from TensorFlow Datasets\n",
        "\n",
        "Now we've downloaded the Food101 dataset from TensorFlow Datasets, how about we do what any good data explorer should?\n",
        "\n",
        "In other words, \"visualize, visualize, visualize\".\n",
        "\n",
        "Let's find out a few details about our dataset:\n",
        "* The shape of our input data (image tensors)\n",
        "* The datatype of our input data\n",
        "* What the labels of our input data look like (e.g. one-hot encoded versus label-encoded)\n",
        "* Do the labels match up with the class names?\n",
        "\n",
        "To do, let's take one sample off the training data (using the [`.take()` method](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take)) and explore it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eO2qVy3A-CC"
      },
      "source": [
        "# Take one sample off the training data\n",
        "train_one_sample = train_data.take(1) # samples are in format (image_tensor, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsZj4K3ETdvB"
      },
      "source": [
        "Because we used the `as_supervised=True` parameter in our `tfds.load()` method above, data samples come in the tuple format structure `(data, label)` or in our case `(image_tensor, label)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m--0wDNDTU8S"
      },
      "source": [
        "# What does one sample of our training data look like?\n",
        "train_one_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP1MeznpTsbM"
      },
      "source": [
        "Let's loop through our single training sample and get some info from the `image_tensor` and `label`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjz4goiHBMO7"
      },
      "source": [
        "# Output info about our training sample\n",
        "for image, label in train_one_sample:\n",
        "  print(f\"\"\"\n",
        "  Image shape: {image.shape}\n",
        "  Image dtype: {image.dtype}\n",
        "  Target class from Food101 (tensor form): {label}\n",
        "  Class name (str form): {class_names[label.numpy()]}\n",
        "        \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4_od8dUUSHE"
      },
      "source": [
        "Because we set the `shuffle_files=True` parameter in our `tfds.load()` method above, running the cell above a few times will give a different result each time.\n",
        "\n",
        "Checking these you might notice some of the images have different shapes, for example `(512, 342, 3)` and `(512, 512, 3)` (height, width, color_channels).\n",
        "\n",
        "Let's see what one of the image tensors from TFDS's Food101 dataset looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuZmVEH-WS4b"
      },
      "source": [
        "# What does an image tensor from TFDS's Food101 look like?\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jJF7njRVKh6"
      },
      "source": [
        "# What are the min and max values?\n",
        "tf.reduce_min(image), tf.reduce_max(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2GvO7HjVF5i"
      },
      "source": [
        "Alright looks like our image tensors have values of between 0 & 255 (standard red, green, blue colour values) and the values are of data type `unit8`.\n",
        "\n",
        "We might have to preprocess these before passing them to a neural network. But we'll handle this later.\n",
        "\n",
        "In the meantime, let's see if we can plot an image sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llQyIBfJWc5x"
      },
      "source": [
        "### Plot an image from TensorFlow Datasets\n",
        "\n",
        "We've seen our image tensors in tensor format, now let's really adhere to our motto.\n",
        "\n",
        "\"Visualize, visualize, visualize!\"\n",
        "\n",
        "Let's plot one of the image samples using [`matplotlib.pyplot.imshow()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) and set the title to target class name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK581hgPWyLm"
      },
      "source": [
        "# Plot an image tensor\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image)\n",
        "plt.title(class_names[label.numpy()]) # add title to image by indexing on class_names list\n",
        "plt.axis(False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mBAtGnPWQHy"
      },
      "source": [
        "Delicious!\n",
        "\n",
        "Okay, looks like the Food101 data we've got from TFDS is similar to the datasets we've been using in previous notebooks.\n",
        "\n",
        "Now let's preprocess it and get it ready for use with a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeRJnQMIYLcy"
      },
      "source": [
        "## Create preprocessing functions for our data\n",
        "\n",
        "In previous notebooks, when our images were in folder format we used the method [`tf.keras.preprocessing.image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) to load them in.\n",
        "\n",
        "Doing this meant our data was loaded into a format ready to be used with our models.\n",
        "\n",
        "However, since we've downloaded the data from TensorFlow Datasets, there are a couple of preprocessing steps we have to take before it's ready to model.\n",
        "\n",
        "More specifically, our data is currently:\n",
        "\n",
        "* In `uint8` data type\n",
        "* Comprised of all differnet sized tensors (different sized images)\n",
        "* Not scaled (the pixel values are between 0 & 255)\n",
        "\n",
        "Whereas, models like data to be:\n",
        "\n",
        "* In `float32` data type\n",
        "* Have all of the same size tensors (batches require all tensors have the same shape, e.g. `(224, 224, 3)`)\n",
        "* Scaled (values between 0 & 1), also called normalized\n",
        "\n",
        "To take care of these, we'll create a `preprocess_img()` function which:\n",
        "\n",
        "* Resizes an input image tensor to a specified size using [`tf.image.resize()`](https://www.tensorflow.org/api_docs/python/tf/image/resize)\n",
        "* Converts an input image tensor's current datatype to `tf.float32` using [`tf.cast()`](https://www.tensorflow.org/api_docs/python/tf/cast)\n",
        "\n",
        "> 🔑 **Note:** Pretrained EfficientNetBX models in [`tf.keras.applications.efficientnet`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet) (what we're going to be using) have rescaling built-in. But for many other model architectures you'll want to rescale your data (e.g. get its values between 0 & 1). This could be incorporated inside your \"`preprocess_img()`\" function (like the one below) or within your model as a [`tf.keras.layers.experimental.preprocessing.Rescaling`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling) layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKuwdjm0CWc1"
      },
      "source": [
        "# Make a function for preprocessing images\n",
        "def preprocess_img(image, label, img_shape=224):\n",
        "  \"\"\"\n",
        "  Converts image datatype from 'uint8' -> 'float32' and reshapes image to\n",
        "  [img_shape, img_shape, color_channels]\n",
        "  \"\"\"\n",
        "  image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape\n",
        "  return tf.cast(image, tf.float32), label # return (float32_image, label) tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6kGGFa1Z3Nz"
      },
      "source": [
        "Our `preprocess_img()` function above takes image and label as input (even though it does nothing to the label) because our dataset is currently in the tuple structure `(image, label)`.\n",
        "\n",
        "Let's try our function out on a target image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqPDUGCvHI4K"
      },
      "source": [
        "# Preprocess a single sample image and check the outputs\n",
        "preprocessed_img = preprocess_img(image, label)[0]\n",
        "print(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape: {image.shape},\\nDatatype: {image.dtype}\\n\")\n",
        "print(f\"Image after preprocessing:\\n {preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape},\\nDatatype: {preprocessed_img.dtype}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhIIvprqaHEZ"
      },
      "source": [
        "Excellent! Looks like our `preprocess_img()` function is working as expected.\n",
        "\n",
        "The input image gets converted from `uint8` to `float32` and gets reshaped from its current shape to `(224, 224, 3)`.\n",
        "\n",
        "How does it look?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYtMxQzZY0F7"
      },
      "source": [
        "# We can still plot our preprocessed image as long as we\n",
        "# divide by 255 (for matplotlib capatibility)\n",
        "plt.imshow(preprocessed_img/255.)\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsIaJZEU7y_M"
      },
      "source": [
        "All this food visualization is making me hungry. How about we start preparing to model it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2rd4_3CjdGE"
      },
      "source": [
        "## Batch & prepare datasets\n",
        "\n",
        "Before we can model our data, we have to turn it into batches.\n",
        "\n",
        "Why?\n",
        "\n",
        "Because computing on batches is memory efficient.\n",
        "\n",
        "We turn our data from 101,000 image tensors and labels (train and test combined) into batches of 32 image and label pairs, thus enabling it to fit into the memory of our GPU.\n",
        "\n",
        "To do this in effective way, we're going to be leveraging a number of methods from the [`tf.data` API](https://www.tensorflow.org/api_docs/python/tf/data).\n",
        "\n",
        "> 📖 **Resource:** For loading data in the most performant way possible, see the TensorFlow docuemntation on [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance).\n",
        "\n",
        "Specifically, we're going to be using:\n",
        "\n",
        "* [`map()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) - maps a predefined function to a target dataset (e.g. `preprocess_img()` to our image tensors)\n",
        "* [`shuffle()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) - randomly shuffles the elements of a target dataset up `buffer_size` (ideally, the `buffer_size` is equal to the size of the dataset, however, this may have implications on memory)\n",
        "* [`batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) - turns elements of a target dataset into batches (size defined by parameter `batch_size`)\n",
        "* [`prefetch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) - prepares subsequent batches of data whilst other batches of data are being computed on (improves data loading speed but costs memory)\n",
        "* Extra: [`cache()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) - caches elements in a target dataset, saving loading time (will only if your dataset is small enough to fit in memory, standard Colab instances only have 12GB of memory)\n",
        "\n",
        "Things to note:\n",
        "- Can't batch tensors of different shapes (e.g. different image sizes, need to reshape images first, hence our `preprocess_img()` function)\n",
        "- `shuffle()` keeps a buffer of the number you pass it images shuffled, ideally this number would be all of the samples in your training set, however, if your training set is large, this buffer might not fit in memory (a fairly large number like 1000 or 10000 is usually suffice for shuffling)\n",
        "- For methods with the `num_parallel_calls` parameter available (such as `map()`), setting it to`num_parallel_calls=tf.data.AUTOTUNE` will parallelize preprocessing and significantly improve speed\n",
        "- Can't use `cache()` unless your dataset can fit in memory\n",
        "\n",
        "Woah, the above is alot. But once we've coded below, it'll start to make sense.\n",
        "\n",
        "We're going to through things in the following order:\n",
        "\n",
        "```\n",
        "Original dataset (e.g. train_data) -> map() -> shuffle() -> batch() -> prefetch() -> PrefetchDataset\n",
        "```\n",
        "\n",
        "This is like saying,\n",
        "\n",
        "> \"Hey, map this preprocessing function across our training dataset, then shuffle a number of elements before batching them together and make sure you prepare new batches (prefetch) whilst the model is looking through the current batch\".\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/07-prefetching-from-hands-on-ml.png)\n",
        "*What happens when you use prefetching (faster) versus what happens when you don't use prefetching (slower). **Source:** Page 422 of [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow Book by Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/).*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhA4gq-pI2W3"
      },
      "source": [
        "# Map preprocessing function to training data (and paralellize)\n",
        "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
        "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Map prepreprocessing function to test data\n",
        "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Turn test data into batches (don't need to shuffle)\n",
        "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnTPWyAhlKO3"
      },
      "source": [
        "And now let's check out what our prepared datasets look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_fBkGqfJFxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecb691b-616d-47a4-bdbf-85ae3c5c65df"
      },
      "source": [
        "train_data, test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n",
              " <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1fxgyWnlQNU"
      },
      "source": [
        "Excellent! Looks like our data is now in tutples of `(image, label)` with datatypes of `(tf.float32, tf.int64)`, just what our model is after.\n",
        "\n",
        "> 🔑 **Note:** You can get away without calling the `prefetch()` method on the end of your datasets, however, you'd probably see significantly slower data loading speeds when building a model. So most of your dataset input pipelines should end with a call to [`prefecth()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch).\n",
        "\n",
        "Onward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj3umnpMvSw8"
      },
      "source": [
        "## Create modelling callbacks\n",
        "\n",
        "Since we're going to be training on a large amount of data and training could take a long time, it's a good idea to set up some modelling callbacks so we be sure of things like our model's training logs being tracked and our model being checkpointed (saved) after various training milestones.\n",
        "\n",
        "To do each of these we'll use the following callbacks:\n",
        "* [`tf.keras.callbacks.TensorBoard()`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) - allows us to keep track of our model's training history so we can inspect it later (**note:** we've created this callback before have imported it from `helper_functions.py` as `create_tensorboard_callback()`)\n",
        "* [`tf.keras.callbacks.ModelCheckpoint()`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) - saves our model's progress at various intervals so we can load it and resuse it later without having to retrain it\n",
        "  * Checkpointing is also helpful so we can start fine-tuning our model at a particular epoch and revert back to a previous state if fine-tuning offers no benefits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyYmxPnlXOwd"
      },
      "source": [
        "# Create TensorBoard callback (already have \"create_tensorboard_callback()\" from a previous notebook)\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create ModelCheckpoint callback to save model's progress\n",
        "checkpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
        "                                                      save_best_only=True, # only save the best weights\n",
        "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
        "                                                      verbose=0) # don't print out whether or not model is being saved\n",
        "\n",
        "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                                                          patience=3,\n",
        "                                                          restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyXlCU50UElG"
      },
      "source": [
        "## Setup mixed precision training\n",
        "\n",
        "We touched on mixed precision training above.\n",
        "\n",
        "However, we didn't quite explain it.\n",
        "\n",
        "Normally, tensors in TensorFlow default to the float32 datatype (unless otherwise specified).\n",
        "\n",
        "In computer science, float32 is also known as [single-precision floating-point format](https://en.wikipedia.org/wiki/Single-precision_floating-point_format). The 32 means it usually occupies 32 bits in computer memory.\n",
        "\n",
        "Your GPU has a limited memory, therefore it can only handle a number of float32 tensors at the same time.\n",
        "\n",
        "This is where mixed precision training comes in.\n",
        "\n",
        "Mixed precision training involves using a mix of float16 and float32 tensors to make better use of your GPU's memory.\n",
        "\n",
        "Can you guess what float16 means?\n",
        "\n",
        "Well, if you thought since float32 meant single-precision floating-point, you might've guessed float16 means [half-precision floating-point format](https://en.wikipedia.org/wiki/Half-precision_floating-point_format). And if you did, you're right! And if not, no trouble, now you know.\n",
        "\n",
        "For tensors in float16 format, each element occupies 16 bits in computer memory.\n",
        "\n",
        "So, where does this leave us?\n",
        "\n",
        "As mentioned before, when using mixed precision training, your model will make use of float32 and float16 data types to use less memory where possible and in turn run faster (using less memory per tensor means more tensors can be computed on simultaneously).\n",
        "\n",
        "As a result, using mixed precision training can improve your performance on modern GPUs (those with a compute capability score of 7.0+) by up to 3x.\n",
        "\n",
        "For a more detailed explanation, I encourage you to read through the [TensorFlow mixed precision guide](https://www.tensorflow.org/guide/mixed_precision) (I'd highly recommend at least checking out the summary).\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/07-mixed-precision-speedup-equals-3x-gpu.png)\n",
        "*Because mixed precision training uses a combination of float32 and float16 data types, you may see up to a 3x speedup on modern GPUs.*\n",
        "\n",
        "> 🔑 **Note:** If your GPU doesn't have a score of over 7.0+ (e.g. P100 in Colab), mixed precision won't work (see: [\"Supported Hardware\"](https://www.tensorflow.org/guide/mixed_precision#supported_hardware) in the mixed precision guide for more).\n",
        "\n",
        "> 📖 **Resource:** If you'd like to learn more about precision in computer science (the detail to which a numerical quantity is expressed by a computer), see the [Wikipedia page](https://en.wikipedia.org/wiki/Precision_(computer_science)) (and accompanying resources).\n",
        "\n",
        "Okay, enough talk, let's see how we can turn on mixed precision training in TensorFlow.\n",
        "\n",
        "The beautiful thing is, the [`tensorflow.keras.mixed_precision`](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/) API has made it very easy for us to get started.\n",
        "\n",
        "First, we'll import the API and then use the [`set_global_policy()`](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/set_global_policy) method to set the *dtype policy* to `\"mixed_float16\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BuEjmlybR7V"
      },
      "source": [
        "# Turn on mixed precision training\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy(policy=\"mixed_float16\") # set global policy to mixed precision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLxlu7VyYoQm"
      },
      "source": [
        "Nice! As long as the GPU you're using has a compute capability of 7.0+ the cell above should run without error.\n",
        "\n",
        "Now we can check the global dtype policy (the policy which will be used by layers in our model) using the [`mixed_precision.global_policy()`](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/global_policy) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzSWJP8KkKae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4d239c-5b09-4ab3-d719-e3596c21440b"
      },
      "source": [
        "mixed_precision.global_policy() # should output \"mixed_float16\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Policy \"mixed_float16\">"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpnAW2ltXCpE"
      },
      "source": [
        "Great, since the global dtype policy is now `\"mixed_float16\"` our model will automatically take advantage of float16 variables where possible and in turn speed up training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA8FBJwwvVoG"
      },
      "source": [
        "## Build feature extraction model\n",
        "\n",
        "Callbacks: ready to roll.\n",
        "\n",
        "Mixed precision: turned on.\n",
        "\n",
        "Let's build a model.\n",
        "\n",
        "Because our dataset is quite large, we're going to move towards fine-tuning an existing pretrained model (EfficienetNetB0).\n",
        "\n",
        "But before we get into fine-tuning, let's set up a feature-extraction model.\n",
        "\n",
        "Recall, the typical order for using transfer learning is:\n",
        "\n",
        "1. Build a feature extraction model (replace the top few layers of a pretrained model)\n",
        "2. Train for a few epochs with lower layers frozen\n",
        "3. Fine-tune if necessary with multiple layers unfrozen\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/07-feature-extraction-then-fine-tune.png)\n",
        "*Before fine-tuning, it's best practice to train a feature extraction model with custom top layers.*\n",
        "\n",
        "To build the feature extraction model (covered in [Transfer Learning in TensorFlow Part 1: Feature extraction](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb)), we'll:\n",
        "* Use `EfficientNetB0` from [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) pre-trained on ImageNet as our base model\n",
        "  * We'll download this without the top layers using `include_top=False` parameter so we can create our own output layers\n",
        "* Freeze the base model layers so we can use the pre-learned patterns the base model has found on ImageNet\n",
        "* Put together the input, base model, pooling and output layers in a [Functional model](https://keras.io/guides/functional_api/)\n",
        "* Compile the Functional model using the Adam optimizer and [sparse categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) as the loss function (since our labels **aren't** one-hot encoded)\n",
        "* Fit the model for 3 epochs using the TensorBoard and ModelCheckpoint callbacks\n",
        "\n",
        "> 🔑 **Note:** Since we're using mixed precision training, our model needs a separate output layer with a hard-coded `dtype=float32`, for example, `layers.Activation(\"softmax\", dtype=tf.float32)`. This ensures the outputs of our model are returned back to the float32 data type which is more numerically stable than the float16 datatype (important for loss calculations). See the [\"Building the model\"](https://www.tensorflow.org/guide/mixed_precision#building_the_model) section in the TensorFlow mixed precision guide for more.\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/07-mixed-precision-code-before-and-after.png)\n",
        "*Turning mixed precision on in TensorFlow with 3 lines of code.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrkWpCzfXKE7"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# Create base model\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False # freeze base model layers\n",
        "\n",
        "# Create Functional model\n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
        "# x = preprocessing.Rescaling(1./255)(x)\n",
        "x = base_model(inputs, training=False) # set base_model to inference mode only\n",
        "x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
        "x = layers.Dense(len(class_names))(x) # want one output neuron per class\n",
        "# Separate activation of output layer so we can output float32 activations\n",
        "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfEG8ud_jsNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7536a76-4ce1-4536-9b91-b2bd3c8197ba"
      },
      "source": [
        "# Check out our model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
            "                                                                 \n",
            " pooling_layer (GlobalAverag  (None, 1280)             0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 101)               129381    \n",
            "                                                                 \n",
            " softmax_float32 (Activation  (None, 101)              0         \n",
            " )                                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,178,952\n",
            "Trainable params: 129,381\n",
            "Non-trainable params: 4,049,571\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIXkEdnNGpKi"
      },
      "source": [
        "Model ready to go!\n",
        "\n",
        "Before we said the mixed precision API will automatically change our layers' dtype policy's to whatever the global dtype policy is (in our case it's `\"mixed_float16\"`).\n",
        "\n",
        "We can check this by iterating through our model's layers and printing layer attributes such as `dtype` and `dtype_policy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk__ebBLHC-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc257fbf-33a2-45f3-cace-9a42023d783f"
      },
      "source": [
        "# Check the dtype_policy attributes of layers in our model\n",
        "for layer in model.layers:\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # Check the dtype policy of layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_layer True float32 <Policy \"float32\">\n",
            "efficientnetb0 False float32 <Policy \"mixed_float16\">\n",
            "pooling_layer True float32 <Policy \"mixed_float16\">\n",
            "dense_1 True float32 <Policy \"mixed_float16\">\n",
            "softmax_float32 True float32 <Policy \"float32\">\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w6Gv6ySfpNY"
      },
      "source": [
        "Going through the above we see:\n",
        "* `layer.name` (str) : a layer's human-readable name, can be defined by the `name` parameter on construction\n",
        "* `layer.trainable` (bool) : whether or not a layer is trainable (all of our layers are trainable except the efficientnetb0 layer since we set it's `trainable` attribute to `False`\n",
        "* `layer.dtype` : the data type a layer stores its variables in\n",
        "* `layer.dtype_policy` : the data type a layer computes in\n",
        "\n",
        "> 🔑 **Note:** A layer can have a dtype of `float32` and a dtype policy of `\"mixed_float16\"` because it stores its variables (weights & biases) in `float32` (more numerically stable), however it computes in `float16` (faster).\n",
        "\n",
        "We can also check the same details for our model's base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL_THJCYGenQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce6e7f3-0582-4d7d-ade5-4ac8550135a8"
      },
      "source": [
        "# Check the layers in the base model and see what dtype policy they're using\n",
        "for layer in model.layers[1].layers[:20]: # only check the first 20 layers to save output space\n",
        "  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_2 False float32 <Policy \"float32\">\n",
            "rescaling_2 False float32 <Policy \"mixed_float16\">\n",
            "normalization_1 False float32 <Policy \"mixed_float16\">\n",
            "rescaling_3 False float32 <Policy \"mixed_float16\">\n",
            "stem_conv_pad False float32 <Policy \"mixed_float16\">\n",
            "stem_conv False float32 <Policy \"mixed_float16\">\n",
            "stem_bn False float32 <Policy \"mixed_float16\">\n",
            "stem_activation False float32 <Policy \"mixed_float16\">\n",
            "block1a_dwconv False float32 <Policy \"mixed_float16\">\n",
            "block1a_bn False float32 <Policy \"mixed_float16\">\n",
            "block1a_activation False float32 <Policy \"mixed_float16\">\n",
            "block1a_se_squeeze False float32 <Policy \"mixed_float16\">\n",
            "block1a_se_reshape False float32 <Policy \"mixed_float16\">\n",
            "block1a_se_reduce False float32 <Policy \"mixed_float16\">\n",
            "block1a_se_expand False float32 <Policy \"mixed_float16\">\n",
            "block1a_se_excite False float32 <Policy \"mixed_float16\">\n",
            "block1a_project_conv False float32 <Policy \"mixed_float16\">\n",
            "block1a_project_bn False float32 <Policy \"mixed_float16\">\n",
            "block2a_expand_conv False float32 <Policy \"mixed_float16\">\n",
            "block2a_expand_bn False float32 <Policy \"mixed_float16\">\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GerkBr7GiDIj"
      },
      "source": [
        "> 🔑 **Note:** The mixed precision API automatically causes layers which can benefit from using the `\"mixed_float16\"` dtype policy to use it. It also prevents layers which shouldn't use it from using it (e.g. the normalization layer at the start of the base model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJz5S66ojyUS"
      },
      "source": [
        "## TODO: Fit the feature extraction model\n",
        "\n",
        "Now that's one good looking model. Let's fit it to our data shall we?\n",
        "\n",
        "Three epochs should be enough for our top layers to adjust their weights enough to our food image data.\n",
        "\n",
        "To save time per epoch, we'll also only validate on 15% of the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v7rXZG-ZkNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae33ced-c5b8-4552-a1c4-adb7b174c794"
      },
      "source": [
        "# Fit the feature extraction model for 3 epochs with tensorboard and model checkpoint callbacks\n",
        "history_101_food_classes_feature_extract = model.fit(train_data,\n",
        "                                                      epochs=3,\n",
        "                                                      steps_per_epoch=len(train_data),\n",
        "                                                      validation_data=test_data,\n",
        "                                                      validation_steps=int(0.15*len(test_data)),\n",
        "                                                      callbacks=[create_tensorboard_callback(dir_name=\"training_logs\",\n",
        "                                                                                             experiment_name=\"efficientnetb0_101_food_classes_feature_extract\"),\n",
        "                                                                 model_checkpoint,\n",
        "                                                                 earlystopping_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientnetb0_101_food_classes_feature_extract/20230511-145325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "2368/2368 [==============================] - 203s 81ms/step - loss: 1.7191 - accuracy: 0.5832 - val_loss: 1.1338 - val_accuracy: 0.6909\n",
            "Epoch 2/3\n",
            "2368/2368 [==============================] - 220s 92ms/step - loss: 1.2000 - accuracy: 0.6884 - val_loss: 1.0429 - val_accuracy: 0.7188\n",
            "Epoch 3/3\n",
            "2368/2368 [==============================] - 206s 86ms/step - loss: 1.0548 - accuracy: 0.7240 - val_loss: 0.9940 - val_accuracy: 0.7275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg01Gh3EnQSu"
      },
      "source": [
        "Nice, looks like our feature extraction model is performing pretty well. How about we evaluate it on the whole test dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhV7fvTreV27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ed8fd8-0228-4284-fe0b-623eaaaf602c"
      },
      "source": [
        "# Evaluate model (unsaved version) on whole test dataset\n",
        "results_101_food_classes_feature_extract = model.evaluate(test_data)\n",
        "results_101_food_classes_feature_extract"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "790/790 [==============================] - 57s 72ms/step - loss: 1.0000 - accuracy: 0.7263\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.999998152256012, 0.726336658000946]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvTGiIFv3eOe"
      },
      "source": [
        "## TODO: Save the whole model to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHKn4Ex57wzF"
      },
      "source": [
        "# Save model locally (if you're using Google Colab, your saved model will Colab instance terminates)\n",
        "# model.save(\"101_food_classes_feature_extract_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKGDBKrU6rej"
      },
      "source": [
        "# Load model previously saved above\n",
        "model.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUJXpxmMKnYV"
      },
      "source": [
        "# Check the layers in the base model and see what dtype policy they're using\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qym9gSm6vL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5080084-df42-4d27-9f8f-88385eaf5c99"
      },
      "source": [
        "# Check loaded model performance (this should be the same as results_feature_extract_model)\n",
        "model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "790/790 [==============================] - 51s 65ms/step - loss: 1.0000 - accuracy: 0.7263\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9999994039535522, 0.726336658000946]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BUhHSXI-l0v"
      },
      "source": [
        "# The loaded model's results should equal (or at least be very close) to the model's results prior to saving\n",
        "# Note: this will only work if you've instatiated results variables\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj21OVxBGlw9"
      },
      "source": [
        "## TODO: Preparing our model's layers for fine-tuning\n",
        "\n",
        "**Next:** Fine-tune the feature extraction model to beat the [DeepFood paper](https://arxiv.org/pdf/1606.05675.pdf).\n",
        "\n",
        "Like all good cooking shows, I've saved a model I prepared earlier (the feature extraction model from above) to Google Storage.\n",
        "\n",
        "You can download it to make sure you're using the same model as originall trained going forward."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True\n",
        "\n",
        "for layer in base_model.layers[:-16]:\n",
        "  layer.trainable = False\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  print(f\"Layer: {layer.name} Trainable: {layer.trainable}\")"
      ],
      "metadata": {
        "id": "m7SMW79YzEG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b21a068-4ac4-4eac-ae55-22f8878e6674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: input_2 Trainable: False\n",
            "Layer: rescaling_2 Trainable: False\n",
            "Layer: normalization_1 Trainable: False\n",
            "Layer: rescaling_3 Trainable: False\n",
            "Layer: stem_conv_pad Trainable: False\n",
            "Layer: stem_conv Trainable: False\n",
            "Layer: stem_bn Trainable: False\n",
            "Layer: stem_activation Trainable: False\n",
            "Layer: block1a_dwconv Trainable: False\n",
            "Layer: block1a_bn Trainable: False\n",
            "Layer: block1a_activation Trainable: False\n",
            "Layer: block1a_se_squeeze Trainable: False\n",
            "Layer: block1a_se_reshape Trainable: False\n",
            "Layer: block1a_se_reduce Trainable: False\n",
            "Layer: block1a_se_expand Trainable: False\n",
            "Layer: block1a_se_excite Trainable: False\n",
            "Layer: block1a_project_conv Trainable: False\n",
            "Layer: block1a_project_bn Trainable: False\n",
            "Layer: block2a_expand_conv Trainable: False\n",
            "Layer: block2a_expand_bn Trainable: False\n",
            "Layer: block2a_expand_activation Trainable: False\n",
            "Layer: block2a_dwconv_pad Trainable: False\n",
            "Layer: block2a_dwconv Trainable: False\n",
            "Layer: block2a_bn Trainable: False\n",
            "Layer: block2a_activation Trainable: False\n",
            "Layer: block2a_se_squeeze Trainable: False\n",
            "Layer: block2a_se_reshape Trainable: False\n",
            "Layer: block2a_se_reduce Trainable: False\n",
            "Layer: block2a_se_expand Trainable: False\n",
            "Layer: block2a_se_excite Trainable: False\n",
            "Layer: block2a_project_conv Trainable: False\n",
            "Layer: block2a_project_bn Trainable: False\n",
            "Layer: block2b_expand_conv Trainable: False\n",
            "Layer: block2b_expand_bn Trainable: False\n",
            "Layer: block2b_expand_activation Trainable: False\n",
            "Layer: block2b_dwconv Trainable: False\n",
            "Layer: block2b_bn Trainable: False\n",
            "Layer: block2b_activation Trainable: False\n",
            "Layer: block2b_se_squeeze Trainable: False\n",
            "Layer: block2b_se_reshape Trainable: False\n",
            "Layer: block2b_se_reduce Trainable: False\n",
            "Layer: block2b_se_expand Trainable: False\n",
            "Layer: block2b_se_excite Trainable: False\n",
            "Layer: block2b_project_conv Trainable: False\n",
            "Layer: block2b_project_bn Trainable: False\n",
            "Layer: block2b_drop Trainable: False\n",
            "Layer: block2b_add Trainable: False\n",
            "Layer: block3a_expand_conv Trainable: False\n",
            "Layer: block3a_expand_bn Trainable: False\n",
            "Layer: block3a_expand_activation Trainable: False\n",
            "Layer: block3a_dwconv_pad Trainable: False\n",
            "Layer: block3a_dwconv Trainable: False\n",
            "Layer: block3a_bn Trainable: False\n",
            "Layer: block3a_activation Trainable: False\n",
            "Layer: block3a_se_squeeze Trainable: False\n",
            "Layer: block3a_se_reshape Trainable: False\n",
            "Layer: block3a_se_reduce Trainable: False\n",
            "Layer: block3a_se_expand Trainable: False\n",
            "Layer: block3a_se_excite Trainable: False\n",
            "Layer: block3a_project_conv Trainable: False\n",
            "Layer: block3a_project_bn Trainable: False\n",
            "Layer: block3b_expand_conv Trainable: False\n",
            "Layer: block3b_expand_bn Trainable: False\n",
            "Layer: block3b_expand_activation Trainable: False\n",
            "Layer: block3b_dwconv Trainable: False\n",
            "Layer: block3b_bn Trainable: False\n",
            "Layer: block3b_activation Trainable: False\n",
            "Layer: block3b_se_squeeze Trainable: False\n",
            "Layer: block3b_se_reshape Trainable: False\n",
            "Layer: block3b_se_reduce Trainable: False\n",
            "Layer: block3b_se_expand Trainable: False\n",
            "Layer: block3b_se_excite Trainable: False\n",
            "Layer: block3b_project_conv Trainable: False\n",
            "Layer: block3b_project_bn Trainable: False\n",
            "Layer: block3b_drop Trainable: False\n",
            "Layer: block3b_add Trainable: False\n",
            "Layer: block4a_expand_conv Trainable: False\n",
            "Layer: block4a_expand_bn Trainable: False\n",
            "Layer: block4a_expand_activation Trainable: False\n",
            "Layer: block4a_dwconv_pad Trainable: False\n",
            "Layer: block4a_dwconv Trainable: False\n",
            "Layer: block4a_bn Trainable: False\n",
            "Layer: block4a_activation Trainable: False\n",
            "Layer: block4a_se_squeeze Trainable: False\n",
            "Layer: block4a_se_reshape Trainable: False\n",
            "Layer: block4a_se_reduce Trainable: False\n",
            "Layer: block4a_se_expand Trainable: False\n",
            "Layer: block4a_se_excite Trainable: False\n",
            "Layer: block4a_project_conv Trainable: False\n",
            "Layer: block4a_project_bn Trainable: False\n",
            "Layer: block4b_expand_conv Trainable: False\n",
            "Layer: block4b_expand_bn Trainable: False\n",
            "Layer: block4b_expand_activation Trainable: False\n",
            "Layer: block4b_dwconv Trainable: False\n",
            "Layer: block4b_bn Trainable: False\n",
            "Layer: block4b_activation Trainable: False\n",
            "Layer: block4b_se_squeeze Trainable: False\n",
            "Layer: block4b_se_reshape Trainable: False\n",
            "Layer: block4b_se_reduce Trainable: False\n",
            "Layer: block4b_se_expand Trainable: False\n",
            "Layer: block4b_se_excite Trainable: False\n",
            "Layer: block4b_project_conv Trainable: False\n",
            "Layer: block4b_project_bn Trainable: False\n",
            "Layer: block4b_drop Trainable: False\n",
            "Layer: block4b_add Trainable: False\n",
            "Layer: block4c_expand_conv Trainable: False\n",
            "Layer: block4c_expand_bn Trainable: False\n",
            "Layer: block4c_expand_activation Trainable: False\n",
            "Layer: block4c_dwconv Trainable: False\n",
            "Layer: block4c_bn Trainable: False\n",
            "Layer: block4c_activation Trainable: False\n",
            "Layer: block4c_se_squeeze Trainable: False\n",
            "Layer: block4c_se_reshape Trainable: False\n",
            "Layer: block4c_se_reduce Trainable: False\n",
            "Layer: block4c_se_expand Trainable: False\n",
            "Layer: block4c_se_excite Trainable: False\n",
            "Layer: block4c_project_conv Trainable: False\n",
            "Layer: block4c_project_bn Trainable: False\n",
            "Layer: block4c_drop Trainable: False\n",
            "Layer: block4c_add Trainable: False\n",
            "Layer: block5a_expand_conv Trainable: False\n",
            "Layer: block5a_expand_bn Trainable: False\n",
            "Layer: block5a_expand_activation Trainable: False\n",
            "Layer: block5a_dwconv Trainable: False\n",
            "Layer: block5a_bn Trainable: False\n",
            "Layer: block5a_activation Trainable: False\n",
            "Layer: block5a_se_squeeze Trainable: False\n",
            "Layer: block5a_se_reshape Trainable: False\n",
            "Layer: block5a_se_reduce Trainable: False\n",
            "Layer: block5a_se_expand Trainable: False\n",
            "Layer: block5a_se_excite Trainable: False\n",
            "Layer: block5a_project_conv Trainable: False\n",
            "Layer: block5a_project_bn Trainable: False\n",
            "Layer: block5b_expand_conv Trainable: False\n",
            "Layer: block5b_expand_bn Trainable: False\n",
            "Layer: block5b_expand_activation Trainable: False\n",
            "Layer: block5b_dwconv Trainable: False\n",
            "Layer: block5b_bn Trainable: False\n",
            "Layer: block5b_activation Trainable: False\n",
            "Layer: block5b_se_squeeze Trainable: False\n",
            "Layer: block5b_se_reshape Trainable: False\n",
            "Layer: block5b_se_reduce Trainable: False\n",
            "Layer: block5b_se_expand Trainable: False\n",
            "Layer: block5b_se_excite Trainable: False\n",
            "Layer: block5b_project_conv Trainable: False\n",
            "Layer: block5b_project_bn Trainable: False\n",
            "Layer: block5b_drop Trainable: False\n",
            "Layer: block5b_add Trainable: False\n",
            "Layer: block5c_expand_conv Trainable: False\n",
            "Layer: block5c_expand_bn Trainable: False\n",
            "Layer: block5c_expand_activation Trainable: False\n",
            "Layer: block5c_dwconv Trainable: False\n",
            "Layer: block5c_bn Trainable: False\n",
            "Layer: block5c_activation Trainable: False\n",
            "Layer: block5c_se_squeeze Trainable: False\n",
            "Layer: block5c_se_reshape Trainable: False\n",
            "Layer: block5c_se_reduce Trainable: False\n",
            "Layer: block5c_se_expand Trainable: False\n",
            "Layer: block5c_se_excite Trainable: False\n",
            "Layer: block5c_project_conv Trainable: False\n",
            "Layer: block5c_project_bn Trainable: False\n",
            "Layer: block5c_drop Trainable: False\n",
            "Layer: block5c_add Trainable: False\n",
            "Layer: block6a_expand_conv Trainable: False\n",
            "Layer: block6a_expand_bn Trainable: False\n",
            "Layer: block6a_expand_activation Trainable: False\n",
            "Layer: block6a_dwconv_pad Trainable: False\n",
            "Layer: block6a_dwconv Trainable: False\n",
            "Layer: block6a_bn Trainable: False\n",
            "Layer: block6a_activation Trainable: False\n",
            "Layer: block6a_se_squeeze Trainable: False\n",
            "Layer: block6a_se_reshape Trainable: False\n",
            "Layer: block6a_se_reduce Trainable: False\n",
            "Layer: block6a_se_expand Trainable: False\n",
            "Layer: block6a_se_excite Trainable: False\n",
            "Layer: block6a_project_conv Trainable: False\n",
            "Layer: block6a_project_bn Trainable: False\n",
            "Layer: block6b_expand_conv Trainable: False\n",
            "Layer: block6b_expand_bn Trainable: False\n",
            "Layer: block6b_expand_activation Trainable: False\n",
            "Layer: block6b_dwconv Trainable: False\n",
            "Layer: block6b_bn Trainable: False\n",
            "Layer: block6b_activation Trainable: False\n",
            "Layer: block6b_se_squeeze Trainable: False\n",
            "Layer: block6b_se_reshape Trainable: False\n",
            "Layer: block6b_se_reduce Trainable: False\n",
            "Layer: block6b_se_expand Trainable: False\n",
            "Layer: block6b_se_excite Trainable: False\n",
            "Layer: block6b_project_conv Trainable: False\n",
            "Layer: block6b_project_bn Trainable: False\n",
            "Layer: block6b_drop Trainable: False\n",
            "Layer: block6b_add Trainable: False\n",
            "Layer: block6c_expand_conv Trainable: False\n",
            "Layer: block6c_expand_bn Trainable: False\n",
            "Layer: block6c_expand_activation Trainable: False\n",
            "Layer: block6c_dwconv Trainable: False\n",
            "Layer: block6c_bn Trainable: False\n",
            "Layer: block6c_activation Trainable: False\n",
            "Layer: block6c_se_squeeze Trainable: False\n",
            "Layer: block6c_se_reshape Trainable: False\n",
            "Layer: block6c_se_reduce Trainable: False\n",
            "Layer: block6c_se_expand Trainable: False\n",
            "Layer: block6c_se_excite Trainable: False\n",
            "Layer: block6c_project_conv Trainable: False\n",
            "Layer: block6c_project_bn Trainable: False\n",
            "Layer: block6c_drop Trainable: False\n",
            "Layer: block6c_add Trainable: False\n",
            "Layer: block6d_expand_conv Trainable: False\n",
            "Layer: block6d_expand_bn Trainable: False\n",
            "Layer: block6d_expand_activation Trainable: False\n",
            "Layer: block6d_dwconv Trainable: False\n",
            "Layer: block6d_bn Trainable: False\n",
            "Layer: block6d_activation Trainable: False\n",
            "Layer: block6d_se_squeeze Trainable: False\n",
            "Layer: block6d_se_reshape Trainable: False\n",
            "Layer: block6d_se_reduce Trainable: False\n",
            "Layer: block6d_se_expand Trainable: False\n",
            "Layer: block6d_se_excite Trainable: False\n",
            "Layer: block6d_project_conv Trainable: False\n",
            "Layer: block6d_project_bn Trainable: False\n",
            "Layer: block6d_drop Trainable: False\n",
            "Layer: block6d_add Trainable: False\n",
            "Layer: block7a_expand_conv Trainable: True\n",
            "Layer: block7a_expand_bn Trainable: True\n",
            "Layer: block7a_expand_activation Trainable: True\n",
            "Layer: block7a_dwconv Trainable: True\n",
            "Layer: block7a_bn Trainable: True\n",
            "Layer: block7a_activation Trainable: True\n",
            "Layer: block7a_se_squeeze Trainable: True\n",
            "Layer: block7a_se_reshape Trainable: True\n",
            "Layer: block7a_se_reduce Trainable: True\n",
            "Layer: block7a_se_expand Trainable: True\n",
            "Layer: block7a_se_excite Trainable: True\n",
            "Layer: block7a_project_conv Trainable: True\n",
            "Layer: block7a_project_bn Trainable: True\n",
            "Layer: top_conv Trainable: True\n",
            "Layer: top_bn Trainable: True\n",
            "Layer: top_activation Trainable: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "TxM1Uq7x1eAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcKFVlXVwjJy"
      },
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "# Monitor the val_loss and stop training if it doesn't improve for 3 epochs\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping for more\n",
        "\n",
        "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                                                          patience=5,\n",
        "                                                          restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "# Save the best model only\n",
        "# Monitor val_loss while training and save the best model (lowest val_loss)\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint for more\n",
        "\n",
        "# Create ModelCheckpoint callback to save model's progress\n",
        "checkpoint_path_1 = \"model_checkpoints/cp_1.ckpt\" # saving weights requires \".ckpt\" extension\n",
        "model_checkpoint_1 = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_1,\n",
        "                                                        monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
        "                                                        save_best_only=True, # only save the best weights\n",
        "                                                        save_weights_only=True, # only save model weights (not whole model)\n",
        "                                                        verbose=0)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
        "                                                 factor=0.25, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkUtOdVkbMPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6372eafb-6005-4834-acb1-0cbe9bab46d0"
      },
      "source": [
        "# Start to fine-tune (all layers)\n",
        "# Use 100 epochs as the default\n",
        "# Validate on 15% of the test_data\n",
        "# Use the create_tensorboard_callback, ModelCheckpoint and EarlyStopping callbacks you created eaelier\n",
        "# Fit the feature extraction model for 3 epochs with tensorboard and model checkpoint callbacks\n",
        "history_101_food_classes_fine_tuning_1 = model.fit(train_data,\n",
        "                                                   epochs=100,\n",
        "                                                   initial_epoch=history_101_food_classes_feature_extract.epoch[-1]+1,\n",
        "                                                   steps_per_epoch=len(train_data),\n",
        "                                                   validation_data=test_data,\n",
        "                                                   validation_steps=int(0.15*len(test_data)),\n",
        "                                                   callbacks=[create_tensorboard_callback(dir_name=\"training_logs\",\n",
        "                                                                                          experiment_name=\"efficientnetb0_101_food_classes_fine_tuning_1\"),\n",
        "                                                              model_checkpoint_1,\n",
        "                                                              earlystopping_callback,\n",
        "                                                              reduce_lr])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientnetb0_101_food_classes_fine_tuning_1/20230511-150833\n",
            "Epoch 4/100\n",
            "2368/2368 [==============================] - 242s 78ms/step - loss: 0.8621 - accuracy: 0.7707 - val_loss: 0.8843 - val_accuracy: 0.7526 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "2368/2368 [==============================] - 184s 77ms/step - loss: 0.6795 - accuracy: 0.8176 - val_loss: 0.8719 - val_accuracy: 0.7572 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "2368/2368 [==============================] - 188s 79ms/step - loss: 0.5389 - accuracy: 0.8550 - val_loss: 0.8897 - val_accuracy: 0.7572 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "2368/2368 [==============================] - 186s 78ms/step - loss: 0.4177 - accuracy: 0.8893 - val_loss: 0.9126 - val_accuracy: 0.7630 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "2368/2368 [==============================] - 184s 77ms/step - loss: 0.3117 - accuracy: 0.9200 - val_loss: 0.9815 - val_accuracy: 0.7611 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "2367/2368 [============================>.] - ETA: 0s - loss: 0.2209 - accuracy: 0.9446\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "2368/2368 [==============================] - 195s 81ms/step - loss: 0.2209 - accuracy: 0.9446 - val_loss: 1.0654 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "2368/2368 [==============================] - 186s 77ms/step - loss: 0.1088 - accuracy: 0.9803 - val_loss: 1.1149 - val_accuracy: 0.7577 - lr: 2.5000e-05\n",
            "Epoch 11/100\n",
            "2367/2368 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9870\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "2368/2368 [==============================] - 184s 77ms/step - loss: 0.0841 - accuracy: 0.9870 - val_loss: 1.1383 - val_accuracy: 0.7593 - lr: 2.5000e-05\n",
            "Epoch 12/100\n",
            "2368/2368 [==============================] - 186s 78ms/step - loss: 0.0592 - accuracy: 0.9934 - val_loss: 1.1776 - val_accuracy: 0.7572 - lr: 6.2500e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_101_food_classes_fine_tuning_1 = model.evaluate(test_data)\n",
        "results_101_food_classes_fine_tuning_1"
      ],
      "metadata": {
        "id": "8KOs4dG-6uca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8b7a64-137e-4241-e94d-1c848c275742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "790/790 [==============================] - 53s 68ms/step - loss: 0.9355 - accuracy: 0.7582\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9354623556137085, 0.7582178115844727]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ASRN2tG847v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  print(f\"Layer: {layer.name} Trainable: {layer.trainable}\")"
      ],
      "metadata": {
        "id": "c6GcZe-P86ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91091c00-7384-4802-918c-8a903cd97e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: input_2 Trainable: True\n",
            "Layer: rescaling_2 Trainable: True\n",
            "Layer: normalization_1 Trainable: True\n",
            "Layer: rescaling_3 Trainable: True\n",
            "Layer: stem_conv_pad Trainable: True\n",
            "Layer: stem_conv Trainable: True\n",
            "Layer: stem_bn Trainable: True\n",
            "Layer: stem_activation Trainable: True\n",
            "Layer: block1a_dwconv Trainable: True\n",
            "Layer: block1a_bn Trainable: True\n",
            "Layer: block1a_activation Trainable: True\n",
            "Layer: block1a_se_squeeze Trainable: True\n",
            "Layer: block1a_se_reshape Trainable: True\n",
            "Layer: block1a_se_reduce Trainable: True\n",
            "Layer: block1a_se_expand Trainable: True\n",
            "Layer: block1a_se_excite Trainable: True\n",
            "Layer: block1a_project_conv Trainable: True\n",
            "Layer: block1a_project_bn Trainable: True\n",
            "Layer: block2a_expand_conv Trainable: True\n",
            "Layer: block2a_expand_bn Trainable: True\n",
            "Layer: block2a_expand_activation Trainable: True\n",
            "Layer: block2a_dwconv_pad Trainable: True\n",
            "Layer: block2a_dwconv Trainable: True\n",
            "Layer: block2a_bn Trainable: True\n",
            "Layer: block2a_activation Trainable: True\n",
            "Layer: block2a_se_squeeze Trainable: True\n",
            "Layer: block2a_se_reshape Trainable: True\n",
            "Layer: block2a_se_reduce Trainable: True\n",
            "Layer: block2a_se_expand Trainable: True\n",
            "Layer: block2a_se_excite Trainable: True\n",
            "Layer: block2a_project_conv Trainable: True\n",
            "Layer: block2a_project_bn Trainable: True\n",
            "Layer: block2b_expand_conv Trainable: True\n",
            "Layer: block2b_expand_bn Trainable: True\n",
            "Layer: block2b_expand_activation Trainable: True\n",
            "Layer: block2b_dwconv Trainable: True\n",
            "Layer: block2b_bn Trainable: True\n",
            "Layer: block2b_activation Trainable: True\n",
            "Layer: block2b_se_squeeze Trainable: True\n",
            "Layer: block2b_se_reshape Trainable: True\n",
            "Layer: block2b_se_reduce Trainable: True\n",
            "Layer: block2b_se_expand Trainable: True\n",
            "Layer: block2b_se_excite Trainable: True\n",
            "Layer: block2b_project_conv Trainable: True\n",
            "Layer: block2b_project_bn Trainable: True\n",
            "Layer: block2b_drop Trainable: True\n",
            "Layer: block2b_add Trainable: True\n",
            "Layer: block3a_expand_conv Trainable: True\n",
            "Layer: block3a_expand_bn Trainable: True\n",
            "Layer: block3a_expand_activation Trainable: True\n",
            "Layer: block3a_dwconv_pad Trainable: True\n",
            "Layer: block3a_dwconv Trainable: True\n",
            "Layer: block3a_bn Trainable: True\n",
            "Layer: block3a_activation Trainable: True\n",
            "Layer: block3a_se_squeeze Trainable: True\n",
            "Layer: block3a_se_reshape Trainable: True\n",
            "Layer: block3a_se_reduce Trainable: True\n",
            "Layer: block3a_se_expand Trainable: True\n",
            "Layer: block3a_se_excite Trainable: True\n",
            "Layer: block3a_project_conv Trainable: True\n",
            "Layer: block3a_project_bn Trainable: True\n",
            "Layer: block3b_expand_conv Trainable: True\n",
            "Layer: block3b_expand_bn Trainable: True\n",
            "Layer: block3b_expand_activation Trainable: True\n",
            "Layer: block3b_dwconv Trainable: True\n",
            "Layer: block3b_bn Trainable: True\n",
            "Layer: block3b_activation Trainable: True\n",
            "Layer: block3b_se_squeeze Trainable: True\n",
            "Layer: block3b_se_reshape Trainable: True\n",
            "Layer: block3b_se_reduce Trainable: True\n",
            "Layer: block3b_se_expand Trainable: True\n",
            "Layer: block3b_se_excite Trainable: True\n",
            "Layer: block3b_project_conv Trainable: True\n",
            "Layer: block3b_project_bn Trainable: True\n",
            "Layer: block3b_drop Trainable: True\n",
            "Layer: block3b_add Trainable: True\n",
            "Layer: block4a_expand_conv Trainable: True\n",
            "Layer: block4a_expand_bn Trainable: True\n",
            "Layer: block4a_expand_activation Trainable: True\n",
            "Layer: block4a_dwconv_pad Trainable: True\n",
            "Layer: block4a_dwconv Trainable: True\n",
            "Layer: block4a_bn Trainable: True\n",
            "Layer: block4a_activation Trainable: True\n",
            "Layer: block4a_se_squeeze Trainable: True\n",
            "Layer: block4a_se_reshape Trainable: True\n",
            "Layer: block4a_se_reduce Trainable: True\n",
            "Layer: block4a_se_expand Trainable: True\n",
            "Layer: block4a_se_excite Trainable: True\n",
            "Layer: block4a_project_conv Trainable: True\n",
            "Layer: block4a_project_bn Trainable: True\n",
            "Layer: block4b_expand_conv Trainable: True\n",
            "Layer: block4b_expand_bn Trainable: True\n",
            "Layer: block4b_expand_activation Trainable: True\n",
            "Layer: block4b_dwconv Trainable: True\n",
            "Layer: block4b_bn Trainable: True\n",
            "Layer: block4b_activation Trainable: True\n",
            "Layer: block4b_se_squeeze Trainable: True\n",
            "Layer: block4b_se_reshape Trainable: True\n",
            "Layer: block4b_se_reduce Trainable: True\n",
            "Layer: block4b_se_expand Trainable: True\n",
            "Layer: block4b_se_excite Trainable: True\n",
            "Layer: block4b_project_conv Trainable: True\n",
            "Layer: block4b_project_bn Trainable: True\n",
            "Layer: block4b_drop Trainable: True\n",
            "Layer: block4b_add Trainable: True\n",
            "Layer: block4c_expand_conv Trainable: True\n",
            "Layer: block4c_expand_bn Trainable: True\n",
            "Layer: block4c_expand_activation Trainable: True\n",
            "Layer: block4c_dwconv Trainable: True\n",
            "Layer: block4c_bn Trainable: True\n",
            "Layer: block4c_activation Trainable: True\n",
            "Layer: block4c_se_squeeze Trainable: True\n",
            "Layer: block4c_se_reshape Trainable: True\n",
            "Layer: block4c_se_reduce Trainable: True\n",
            "Layer: block4c_se_expand Trainable: True\n",
            "Layer: block4c_se_excite Trainable: True\n",
            "Layer: block4c_project_conv Trainable: True\n",
            "Layer: block4c_project_bn Trainable: True\n",
            "Layer: block4c_drop Trainable: True\n",
            "Layer: block4c_add Trainable: True\n",
            "Layer: block5a_expand_conv Trainable: True\n",
            "Layer: block5a_expand_bn Trainable: True\n",
            "Layer: block5a_expand_activation Trainable: True\n",
            "Layer: block5a_dwconv Trainable: True\n",
            "Layer: block5a_bn Trainable: True\n",
            "Layer: block5a_activation Trainable: True\n",
            "Layer: block5a_se_squeeze Trainable: True\n",
            "Layer: block5a_se_reshape Trainable: True\n",
            "Layer: block5a_se_reduce Trainable: True\n",
            "Layer: block5a_se_expand Trainable: True\n",
            "Layer: block5a_se_excite Trainable: True\n",
            "Layer: block5a_project_conv Trainable: True\n",
            "Layer: block5a_project_bn Trainable: True\n",
            "Layer: block5b_expand_conv Trainable: True\n",
            "Layer: block5b_expand_bn Trainable: True\n",
            "Layer: block5b_expand_activation Trainable: True\n",
            "Layer: block5b_dwconv Trainable: True\n",
            "Layer: block5b_bn Trainable: True\n",
            "Layer: block5b_activation Trainable: True\n",
            "Layer: block5b_se_squeeze Trainable: True\n",
            "Layer: block5b_se_reshape Trainable: True\n",
            "Layer: block5b_se_reduce Trainable: True\n",
            "Layer: block5b_se_expand Trainable: True\n",
            "Layer: block5b_se_excite Trainable: True\n",
            "Layer: block5b_project_conv Trainable: True\n",
            "Layer: block5b_project_bn Trainable: True\n",
            "Layer: block5b_drop Trainable: True\n",
            "Layer: block5b_add Trainable: True\n",
            "Layer: block5c_expand_conv Trainable: True\n",
            "Layer: block5c_expand_bn Trainable: True\n",
            "Layer: block5c_expand_activation Trainable: True\n",
            "Layer: block5c_dwconv Trainable: True\n",
            "Layer: block5c_bn Trainable: True\n",
            "Layer: block5c_activation Trainable: True\n",
            "Layer: block5c_se_squeeze Trainable: True\n",
            "Layer: block5c_se_reshape Trainable: True\n",
            "Layer: block5c_se_reduce Trainable: True\n",
            "Layer: block5c_se_expand Trainable: True\n",
            "Layer: block5c_se_excite Trainable: True\n",
            "Layer: block5c_project_conv Trainable: True\n",
            "Layer: block5c_project_bn Trainable: True\n",
            "Layer: block5c_drop Trainable: True\n",
            "Layer: block5c_add Trainable: True\n",
            "Layer: block6a_expand_conv Trainable: True\n",
            "Layer: block6a_expand_bn Trainable: True\n",
            "Layer: block6a_expand_activation Trainable: True\n",
            "Layer: block6a_dwconv_pad Trainable: True\n",
            "Layer: block6a_dwconv Trainable: True\n",
            "Layer: block6a_bn Trainable: True\n",
            "Layer: block6a_activation Trainable: True\n",
            "Layer: block6a_se_squeeze Trainable: True\n",
            "Layer: block6a_se_reshape Trainable: True\n",
            "Layer: block6a_se_reduce Trainable: True\n",
            "Layer: block6a_se_expand Trainable: True\n",
            "Layer: block6a_se_excite Trainable: True\n",
            "Layer: block6a_project_conv Trainable: True\n",
            "Layer: block6a_project_bn Trainable: True\n",
            "Layer: block6b_expand_conv Trainable: True\n",
            "Layer: block6b_expand_bn Trainable: True\n",
            "Layer: block6b_expand_activation Trainable: True\n",
            "Layer: block6b_dwconv Trainable: True\n",
            "Layer: block6b_bn Trainable: True\n",
            "Layer: block6b_activation Trainable: True\n",
            "Layer: block6b_se_squeeze Trainable: True\n",
            "Layer: block6b_se_reshape Trainable: True\n",
            "Layer: block6b_se_reduce Trainable: True\n",
            "Layer: block6b_se_expand Trainable: True\n",
            "Layer: block6b_se_excite Trainable: True\n",
            "Layer: block6b_project_conv Trainable: True\n",
            "Layer: block6b_project_bn Trainable: True\n",
            "Layer: block6b_drop Trainable: True\n",
            "Layer: block6b_add Trainable: True\n",
            "Layer: block6c_expand_conv Trainable: True\n",
            "Layer: block6c_expand_bn Trainable: True\n",
            "Layer: block6c_expand_activation Trainable: True\n",
            "Layer: block6c_dwconv Trainable: True\n",
            "Layer: block6c_bn Trainable: True\n",
            "Layer: block6c_activation Trainable: True\n",
            "Layer: block6c_se_squeeze Trainable: True\n",
            "Layer: block6c_se_reshape Trainable: True\n",
            "Layer: block6c_se_reduce Trainable: True\n",
            "Layer: block6c_se_expand Trainable: True\n",
            "Layer: block6c_se_excite Trainable: True\n",
            "Layer: block6c_project_conv Trainable: True\n",
            "Layer: block6c_project_bn Trainable: True\n",
            "Layer: block6c_drop Trainable: True\n",
            "Layer: block6c_add Trainable: True\n",
            "Layer: block6d_expand_conv Trainable: True\n",
            "Layer: block6d_expand_bn Trainable: True\n",
            "Layer: block6d_expand_activation Trainable: True\n",
            "Layer: block6d_dwconv Trainable: True\n",
            "Layer: block6d_bn Trainable: True\n",
            "Layer: block6d_activation Trainable: True\n",
            "Layer: block6d_se_squeeze Trainable: True\n",
            "Layer: block6d_se_reshape Trainable: True\n",
            "Layer: block6d_se_reduce Trainable: True\n",
            "Layer: block6d_se_expand Trainable: True\n",
            "Layer: block6d_se_excite Trainable: True\n",
            "Layer: block6d_project_conv Trainable: True\n",
            "Layer: block6d_project_bn Trainable: True\n",
            "Layer: block6d_drop Trainable: True\n",
            "Layer: block6d_add Trainable: True\n",
            "Layer: block7a_expand_conv Trainable: True\n",
            "Layer: block7a_expand_bn Trainable: True\n",
            "Layer: block7a_expand_activation Trainable: True\n",
            "Layer: block7a_dwconv Trainable: True\n",
            "Layer: block7a_bn Trainable: True\n",
            "Layer: block7a_activation Trainable: True\n",
            "Layer: block7a_se_squeeze Trainable: True\n",
            "Layer: block7a_se_reshape Trainable: True\n",
            "Layer: block7a_se_reduce Trainable: True\n",
            "Layer: block7a_se_expand Trainable: True\n",
            "Layer: block7a_se_excite Trainable: True\n",
            "Layer: block7a_project_conv Trainable: True\n",
            "Layer: block7a_project_bn Trainable: True\n",
            "Layer: top_conv Trainable: True\n",
            "Layer: top_bn Trainable: True\n",
            "Layer: top_activation Trainable: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "vXfU-OpTXVL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uptr4mDS86lu"
      },
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "# Monitor the val_loss and stop training if it doesn't improve for 3 epochs\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping for more\n",
        "\n",
        "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                                                          patience=5,\n",
        "                                                          restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "# Save the best model only\n",
        "# Monitor val_loss while training and save the best model (lowest val_loss)\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint for more\n",
        "\n",
        "# Create ModelCheckpoint callback to save model's progress\n",
        "checkpoint_path_2 = \"model_checkpoints/cp_2.ckpt\" # saving weights requires \".ckpt\" extension\n",
        "model_checkpoint_2 = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_2,\n",
        "                                                        monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
        "                                                        save_best_only=True, # only save the best weights\n",
        "                                                        save_weights_only=True, # only save model weights (not whole model)\n",
        "                                                        verbose=0)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
        "                                                 factor=0.25, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start to fine-tune (all layers)\n",
        "# Use 100 epochs as the default\n",
        "# Validate on 15% of the test_data\n",
        "# Use the create_tensorboard_callback, ModelCheckpoint and EarlyStopping callbacks you created eaelier\n",
        "# Fit the feature extraction model for 3 epochs with tensorboard and model checkpoint callbacks\n",
        "history_101_food_classes_fine_tuning_2 = model.fit(train_data,\n",
        "                                                   epochs=100,\n",
        "                                                   initial_epoch=history_101_food_classes_fine_tuning_1.epoch[-1]+1,\n",
        "                                                   steps_per_epoch=len(train_data),\n",
        "                                                   validation_data=test_data,\n",
        "                                                   validation_steps=int(0.15*len(test_data)),\n",
        "                                                   callbacks=[create_tensorboard_callback(dir_name=\"training_logs\",\n",
        "                                                                                          experiment_name=\"efficientnetb0_101_food_classes_fine_tuning_2\"),\n",
        "                                                              model_checkpoint_2,\n",
        "                                                              earlystopping_callback,\n",
        "                                                              reduce_lr])"
      ],
      "metadata": {
        "id": "Q2M4RKsSaF3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b651f1-bc8b-44eb-9c3a-23160c10be7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientnetb0_101_food_classes_fine_tuning_2/20230511-153936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100\n",
            "2368/2368 [==============================] - 421s 155ms/step - loss: 0.2608 - accuracy: 0.9355 - val_loss: 0.9783 - val_accuracy: 0.7704 - lr: 2.0000e-05\n",
            "Epoch 14/100\n",
            "2368/2368 [==============================] - 363s 152ms/step - loss: 0.1772 - accuracy: 0.9584 - val_loss: 1.0423 - val_accuracy: 0.7651 - lr: 2.0000e-05\n",
            "Epoch 15/100\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 0.1168 - accuracy: 0.9755\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "2368/2368 [==============================] - 368s 154ms/step - loss: 0.1168 - accuracy: 0.9755 - val_loss: 1.1500 - val_accuracy: 0.7646 - lr: 2.0000e-05\n",
            "Epoch 16/100\n",
            "2368/2368 [==============================] - 368s 154ms/step - loss: 0.0568 - accuracy: 0.9916 - val_loss: 1.2217 - val_accuracy: 0.7722 - lr: 5.0000e-06\n",
            "Epoch 17/100\n",
            "2368/2368 [==============================] - 366s 153ms/step - loss: 0.0447 - accuracy: 0.9942 - val_loss: 1.2814 - val_accuracy: 0.7685 - lr: 5.0000e-06\n",
            "Epoch 18/100\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9964\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
            "2368/2368 [==============================] - 367s 154ms/step - loss: 0.0342 - accuracy: 0.9964 - val_loss: 1.3525 - val_accuracy: 0.7675 - lr: 5.0000e-06\n",
            "Epoch 19/100\n",
            "2368/2368 [==============================] - 365s 153ms/step - loss: 0.0237 - accuracy: 0.9983 - val_loss: 1.3556 - val_accuracy: 0.7720 - lr: 1.2500e-06\n",
            "Epoch 20/100\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9985\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
            "2368/2368 [==============================] - 366s 153ms/step - loss: 0.0216 - accuracy: 0.9985 - val_loss: 1.4046 - val_accuracy: 0.7669 - lr: 1.2500e-06\n",
            "Epoch 21/100\n",
            " 815/2368 [=========>....................] - ETA: 3:54 - loss: 0.0212 - accuracy: 0.9986"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnKo5ckb86lu"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_101_food_classes_fine_tuning_2 = model.evaluate(test_data)\n",
        "results_101_food_classes_fine_tuning_2"
      ],
      "metadata": {
        "id": "CnQRPqmC86lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = False\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "_-s2fX2WlXWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  print(f\"Layer: {layer.name} Trainable: {layer.trainable}\")"
      ],
      "metadata": {
        "id": "POloD-J1lXna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "LCGpa6qZlXnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bdk7iWWlXnb"
      },
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "# Monitor the val_loss and stop training if it doesn't improve for 3 epochs\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping for more\n",
        "\n",
        "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                                                          patience=5,\n",
        "                                                          restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "# Save the best model only\n",
        "# Monitor val_loss while training and save the best model (lowest val_loss)\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint for more\n",
        "\n",
        "# Create ModelCheckpoint callback to save model's progress\n",
        "checkpoint_path_3 = \"model_checkpoints/cp_3.ckpt\" # saving weights requires \".ckpt\" extension\n",
        "model_checkpoint_3 = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_3,\n",
        "                                                        monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
        "                                                        save_best_only=True, # only save the best weights\n",
        "                                                        save_weights_only=True, # only save model weights (not whole model)\n",
        "                                                        verbose=0)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
        "                                                 factor=0.25, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start to fine-tune (all layers)\n",
        "# Use 100 epochs as the default\n",
        "# Validate on 15% of the test_data\n",
        "# Use the create_tensorboard_callback, ModelCheckpoint and EarlyStopping callbacks you created eaelier\n",
        "# Fit the feature extraction model for 3 epochs with tensorboard and model checkpoint callbacks\n",
        "history_101_food_classes_fine_tuning_3 = model.fit(train_data,\n",
        "                                                   epochs=100,\n",
        "                                                   initial_epoch=history_101_food_classes_feature_extract.epoch[-1]+1,\n",
        "                                                   steps_per_epoch=len(train_data),\n",
        "                                                   validation_data=test_data,\n",
        "                                                   validation_steps=int(0.15*len(test_data)),\n",
        "                                                   callbacks=[create_tensorboard_callback(dir_name=\"training_logs\",\n",
        "                                                                                          experiment_name=\"efficientnetb0_101_food_classes_fine_tuning_3\"),\n",
        "                                                              model_checkpoint_3,\n",
        "                                                              earlystopping_callback,\n",
        "                                                              reduce_lr])"
      ],
      "metadata": {
        "id": "0G6XYRXQlXnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_101_food_classes_fine_tuning_3 = model.evaluate(test_data)\n",
        "results_101_food_classes_fine_tuning_3"
      ],
      "metadata": {
        "id": "9qINvSNxlXnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1As0OhYHFX-"
      },
      "source": [
        "# Save model locally (note: if you're using Google Colab and you save your model locally, it will be deleted when your Google Colab session ends)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CR6q8MYM37K"
      },
      "source": [
        "# Evaluate mixed precision trained fine-tuned model (this should beat DeepFood's 77.4% top-1 accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFjooc5Gy0I2"
      },
      "source": [
        "## TODO: View training results on TensorBoard\n",
        "\n",
        "**To do:** Upload and view your model's training results to TensorBoard.dev and view them.\n",
        "\n",
        "See https://tensorboard.dev/ for more.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCpVz0GSPucB"
      },
      "source": [
        "!tensorboard dev upload --logdir training_logs \\\n",
        "    --name \"Food Vision Big Project\" \\\n",
        "    --description \"all classes different models\"\\\n",
        "    --one_shot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE17Yr3POZSH"
      },
      "source": [
        "## TODO: Evaluate your trained model\n",
        "\n",
        "Some ideas you might want to go through:\n",
        "1. Find the precision, recall and f1 scores for each class (all 101).\n",
        "2. Build a confusion matrix for each of the classes.\n",
        "3. Find your model's *most wrong* predictions (those with the highest prediction probability but the wrong prediction).\n",
        "\n",
        "See the evaluation section at the end of [Transfer Learning Part 3: Scaling Up for more](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84H_oXV8PqSy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3lDriNpY_UB"
      },
      "outputs": [],
      "source": [
        "from helper_functions import make_confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_labels = []\n",
        "pred_classes = []\n",
        "for images, labels in test_data.unbatch():\n",
        "  pred_classes.append(model.predict(images, verbose=0).argmax(axis=1))\n",
        "  y_labels.append(labels.numpy())\n",
        "\n",
        "y_labels[:10], pred_classes[:10]"
      ],
      "metadata": {
        "id": "i4mTniXaEeqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWQWe1KXZI7w"
      },
      "outputs": [],
      "source": [
        "cm_model = make_confusion_matrix(y_true=y_labels,\n",
        "                      y_pred=pred_classes,\n",
        "                      classes=class_names,\n",
        "                      figsize=(100, 100),\n",
        "                      text_size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAVCRga7Z-r6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):\n",
        "  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
        "\n",
        "  If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
        "  will be used.\n",
        "\n",
        "  Args:\n",
        "    y_true: Array of truth labels (must be same shape as y_pred).\n",
        "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
        "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
        "    figsize: Size of output figure (default=(10, 10)).\n",
        "    text_size: Size of output figure text (default=15).\n",
        "    norm: normalize values or not (default=False).\n",
        "    savefig: save confusion matrix to file (default=False).\n",
        "\n",
        "  Returns:\n",
        "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
        "\n",
        "  Example usage:\n",
        "    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
        "                          y_pred=y_preds, # predicted labels\n",
        "                          classes=class_names, # array of class label names\n",
        "                          figsize=(15, 15),\n",
        "                          text_size=10)\n",
        "  \"\"\"\n",
        "  # Create the confustion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
        "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
        "\n",
        "  # Plot the figure and make it pretty\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  # Are there a list of classes?\n",
        "  if classes:\n",
        "    labels = classes\n",
        "  else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "\n",
        "  # Label the axes\n",
        "  ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
        "         yticks=np.arange(n_classes),\n",
        "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
        "         yticklabels=labels)\n",
        "\n",
        "  # Make x-axis labels appear on bottom\n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  ## CHANGES\n",
        "  plt.xticks(rotation=70, fontsize=text_size)\n",
        "  plt.yticks(fontsize=text_size)\n",
        "\n",
        "  # Set the threshold for different colors\n",
        "  threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "  # Plot the text on each cell\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    if norm:\n",
        "      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "    else:\n",
        "      plt.text(j, i, f\"{cm[i, j]}\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "\n",
        "  # Save the figure to the current working directory\n",
        "  if savefig:\n",
        "    fig.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "  return cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hETN0hScU3R"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true=y_labels,\n",
        "                            y_pred=pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjkdj0XzdBPP"
      },
      "outputs": [],
      "source": [
        "classification_report_dict = classification_report(y_labels, pred_classes, output_dict=True)\n",
        "classification_report_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSA1BTUxdg9E"
      },
      "outputs": [],
      "source": [
        "class_f1_scores = {}\n",
        "\n",
        "for k, v in classification_report_dict.items():\n",
        "  if k == \"accuracy\":\n",
        "    break\n",
        "  else:\n",
        "    class_f1_scores[test_data.class_names[int(k)]] = v[\"f1-score\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ7sNB2IebHf"
      },
      "outputs": [],
      "source": [
        "class_f1_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IorrdK4el7O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "f1_scores = pd.DataFrame({\"class_name\": list(class_f1_scores.keys()),\n",
        "                          \"f1-score\": list(class_f1_scores.values())}).sort_values(\"f1-score\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vswQqmiSe7Wf"
      },
      "outputs": [],
      "source": [
        "f1_scores[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwEoKZ8Nf7J8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,25))\n",
        "scores = ax.barh(range(len(f1_scores)), f1_scores['f1-score'].values)\n",
        "ax.set_yticks(range(len(f1_scores)))\n",
        "ax.set_yticklabels(f1_scores['class_name'].values)\n",
        "ax.set_xlabel(\"F1-score\")\n",
        "ax.set_title(\"F1-scores for 101 classes\")\n",
        "ax.invert_yaxis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dnR10tLhrjZ"
      },
      "outputs": [],
      "source": [
        "pred_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7q_TfIwFa-q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
        "  \"\"\"\n",
        "  Reads in an image from filename, turns it into a tensor and reshapes into\n",
        "  (224, 224, 3).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename (str): string filename of target image\n",
        "  img_shape (int): size to resize target image to, default 224\n",
        "  scale (bool): whether to scale pixel values to range(0, 1), default True\n",
        "  \"\"\"\n",
        "  # Read in the image\n",
        "  img = tf.io.read_file(filename)\n",
        "  # Decode it into a tensor\n",
        "  img = tf.io.decode_image(img)\n",
        "  # Resize the image\n",
        "  img = tf.image.resize(img, [img_shape, img_shape])\n",
        "  if scale:\n",
        "    # Rescale the image (get all values between 0 and 1)\n",
        "    return img/255.\n",
        "  else:\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNiZyb5QFcSq"
      },
      "outputs": [],
      "source": [
        "class_names = test_data.class_names\n",
        "\n",
        "# Make preds on a series of random images\n",
        "import os\n",
        "import random\n",
        "\n",
        "plt.figure(figsize=(17, 10))\n",
        "for i in range(3):\n",
        "  # Choose a random image from a random class\n",
        "  class_name = random.choice(class_names)\n",
        "  filename = random.choice(os.listdir(test_dir + \"/\" + class_name))\n",
        "  filepath = test_dir + class_name + \"/\" + filename\n",
        "\n",
        "  # Load the image and make predictions\n",
        "  img = load_and_prep_image(filepath, scale=False) # don't scale images for EfficientNet predictions\n",
        "  pred_prob = model.predict(tf.expand_dims(img, axis=0)) # model accepts tensors of shape [None, 224, 224, 3]\n",
        "  pred_class = class_names[pred_prob.argmax()] # find the predicted class\n",
        "\n",
        "  # Plot the image(s)\n",
        "  plt.subplot(1, 3, i+1)\n",
        "  plt.imshow(img/255.)\n",
        "  if class_name == pred_class: # Change the color of text based on whether prediction is right or wrong\n",
        "    title_color = \"g\"\n",
        "  else:\n",
        "    title_color = \"r\"\n",
        "  plt.title(f\"actual: {class_name}, pred: {pred_class}, prob: {pred_prob.max():.2f}\", c=title_color)\n",
        "  plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ6CrHggGxr5"
      },
      "outputs": [],
      "source": [
        "filepaths = []\n",
        "\n",
        "for path in test_data.list_files(\"/content/101_food_classes_10_percent/test/*/*.jpg\",\n",
        "                                 shuffle=False):\n",
        "  filepaths.append(path.numpy())\n",
        "\n",
        "filepaths[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhGl3rwPHcD3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pred_df = pd.DataFrame({\"img_path\":filepaths,\n",
        "                        \"y_true\": y_labels,\n",
        "                        \"y_pred\": pred_classes,\n",
        "                        \"pred_conf\": preds_probs.max(axis=1),\n",
        "                        \"y_true_classname\": [class_names[i] for i in y_labels],\n",
        "                        \"y_pred_classname\": [class_names[i] for i in pred_classes]})\n",
        "\n",
        "pred_df[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy1dSPemIwZL"
      },
      "outputs": [],
      "source": [
        "pred_df[\"pred_correct\"] = pred_df[\"y_true_classname\"] == pred_df[\"y_pred_classname\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbTmf0sJJAG5"
      },
      "outputs": [],
      "source": [
        "pred_df[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gFBxI7DJGuY"
      },
      "outputs": [],
      "source": [
        "top_100_wrong = pred_df[pred_df['pred_correct'] == False].sort_values('pred_conf', ascending=False)\n",
        "\n",
        "top_100_wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEGp_buhKiV3"
      },
      "outputs": [],
      "source": [
        "images_to_view = 9\n",
        "start_index = 0\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, row in enumerate(top_100_wrong[start_index:start_index+images_to_view].itertuples()):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  img = load_and_prep_image(row[1], scale=False)\n",
        "  _, _, _, _, pred_prob, y_true_classname, y_pred_classname, _ = row\n",
        "  plt.imshow(img/255.)\n",
        "  plt.title(f\"actual: {y_true_classname}, pred: {y_pred_classname}\\nprob: {pred_prob}\")\n",
        "  plt.axis(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDDrrqy5egTn"
      },
      "source": [
        "## 🛠 Exercises\n",
        "\n",
        "1. Use the same evaluation techniques on the large-scale Food Vision model as you did in the previous notebook ([Transfer Learning Part 3: Scaling up](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb)). More specifically, it would be good to see:\n",
        "  * A confusion matrix between all of the model's predictions and true labels.\n",
        "  * A graph showing the f1-scores of each class.\n",
        "  * A visualization of the model making predictions on various images and comparing the predictions to the ground truth.\n",
        "    * For example, plot a sample image from the test dataset and have the title of the plot show the prediction, the prediction probability and the ground truth label.\n",
        "2. Take 3 of your own photos of food and use the Food Vision model to make predictions on them. How does it go? Share your images/predictions with the other students.\n",
        "3. Retrain the model (feature extraction and fine-tuning) we trained in this notebook, except this time use [`EfficientNetB4`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB4) as the base model instead of `EfficientNetB0`. Do you notice an improvement in performance? Does it take longer to train? Are there any tradeoffs to consider?\n",
        "4. Name one important benefit of mixed precision training, how does this benefit take place?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8ncI458ZkpA"
      },
      "source": [
        "## 📖 Extra-curriculum\n",
        "\n",
        "* Read up on learning rate scheduling and the [learning rate scheduler callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler). What is it? And how might it be helpful to this project?\n",
        "* Read up on TensorFlow data loaders ([improving TensorFlow data loading performance](https://www.tensorflow.org/guide/data_performance)). Is there anything we've missed? What methods you keep in mind whenever loading data in TensorFlow? Hint: check the summary at the bottom of the page for a gret round up of ideas.\n",
        "* Read up on the documentation for [TensorFlow mixed precision training](https://www.tensorflow.org/guide/mixed_precision). What are the important things to keep in mind when using mixed precision training?"
      ]
    }
  ]
}